# K-最近傍法 (K-Nearest Neighbors, K-NN) アルゴリズム

## アルゴリズムの概要

K-最近傍法（K-Nearest Neighbors, 以下k-NN）は、分類と回帰に使用される非パラメトリックな機械学習のアルゴリズム。k-NNの主要な原則は、新しいデータポイントを既存のデータポイントの「最近傍」に基づいて分類または予測するというもの。ここで「最近傍」は、ユークリッド距離やマンハッタン距離など、様々な距離計測方法に基づいて定義される。

## アルゴリズムの動作

1. **距離計算**：新しいデータ点と既存の全てのデータ点との間の距離を計算する。距離の計算は、通常ユークリッド距離やマンハッタン距離を使用。

2. **最近傍の選択**：計算された距離に基づいて、最も近いk個のデータ点（最近傍）を選択する。

3. **分類**：分類問題の場合、これらk個の最近傍の最頻値（つまり、最も一般的なクラス）が新しいデータ点のクラスとして選ばれる。

4. **予測**：回帰問題の場合、これらk個の最近傍の平均値が新しいデータ点の予測値として計算される。

## 特徴と利点

* k-NNは理解しやすく、実装が容易なアルゴリズム。
* 訓練フェーズがほとんど必要なく、すべての計算は分類または予測が必要なときに行われるため、新しい訓練データの追加が容易。

## 注意点

* k-NNは「怠惰な学習者」であり、訓練データセット全体をメモリに保持するため、大規模なデータセットでは効率が悪くなる可能性がある。
* 高次元のデータ（次元の呪い）や、カテゴリ変数の取り扱いが困難な場合もある。
* kの選択は非常に重要で、これによってモデルの性能が大きく影響を受ける。小さすぎるkはノイズの影響を大きく受け、大きすぎるkは決定境界が不明確になる可能性がある。

## パラメータ調整

k-NNの主要なパラメータは、最近傍の数を示すkと、距離計測法である。

* **k**：kは通常、交差検証を使用して選択されます。kが小さい場合、過学習のリスクがあり、kが大きすぎると、未学習のリスクがある。kの適切な選択は、モデルのバイアスとバリアンスの適切なトレードオフを達成する。

* **距離計測法**：ユークリッド距離やマンハッタン距離など、データの種類と特性によって最適な距離計測法が変わる場合がある。

## k-NNの使用例

k-NNは、手書きの数字の認識、画像の認識、ビデオの認識など、さまざまなパターン認識のタスクで使用される。また、推奨システムでも使用される。これは、ユーザーがアイテムを好む可能性が高いと判断するために、そのユーザーと「最も近い」ユーザー（同様の評価パターンを持つユーザー）を見つけるため。
