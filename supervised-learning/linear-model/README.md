## 線形回帰

線形回帰は、一つまたはそれ以上の独立変数（特徴量）と従属変数（目的変数）の間の線形関係をモデリングする。線形回帰の一般形は次のように表される：

```
y = Xβ + ε
```

ここで、`y`は従属変数、`X`は独立変数（特徴量行列）、`β`は回帰係数を表すベクトル、そして`ε`は誤差項である。

### 通常最小二乗法（Ordinary Least Squares, OLS）

通常最小二乗法は、残差平方和（すなわち、実際の値と予測値との差の二乗）が最小となるように、回帰係数を求める。これは線形回帰の最も基本的な形式で、独立変数間の多重共線性やデータのスケーリングなどに影響を受けやすい。

### リッジ回帰（Ridge Regression）

リッジ回帰は、OLSを拡張した形式で、L2正則化項（回帰係数の二乗の和）が残差平方和に加えられる。これにより、回帰係数の絶対値が大きくなりすぎるのを防ぎ、モデルの過学習を抑制する。リッジ回帰は、独立変数間に多重共線性がある場合に特に有効である。

### Lasso回帰（Least Absolute Shrinkage and Selection Operator, Lasso）

Lasso回帰もOLSを拡張した形式で、こちらはL1正則化項（回帰係数の絶対値の和）が残差平方和に加えられる。これにより、一部の回帰係数が完全に0になる可能性がある。そのため、Lasso回帰は特徴量選択に有効で、不要な特徴量（つまり、その回帰係数が0になる特徴量）を自動的に除外することができる。

これらのモデルの選択は、問題の具体的な状況やデータの性質に依存する。例えば、特徴量間に強い相関がある場合や特徴量の数が多い場合は、リッジ回帰やLasso回帰がより適切であることがある。