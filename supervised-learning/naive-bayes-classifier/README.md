# naive bayes classfier(ナイーブベイズ分類器)

## naive bays classfier とは

ベイズの定理に基づき、特徴ベクトルとクラスの条件付き確率を使用してクラスを推定する統計的な分類手法。

naive bayes の 「naive」とは、各特徴の値が独立であるという仮定のこと。この仮定は現実のデータでは当てはまらないことが多いが、計算を単純化し効率化するために採用されている。

## 特徴

- 学習と予測の速度が高速である。
- 相対的に少ないトレーニングデータでも良好なパフォーマンスを示すことがある。
- 特徴の独立性に関する強い仮定を持つため、特徴間の依存関係を捉えることができない。
- 高次元データに適する
- 数値データやカテゴリカルデータの両方に適用可能であり、複数のクラスに拡張することもできる。

## 数学的背景

数学的な説明をするために、以下の記号を使用する

- $X = (X_1, X_2, ..., X_n)$: 入力データの特徴ベクトル
- $C_k$: クラス$k$に属する事例
- $P(C_k|X)$: 特徴ベクトル$X$が与えられた条件下でクラス$C_k$が真である確率
- $P(C_k)$: クラス$C_k$が真である事前確率
- $P(X|C_k)$: クラス$C_k$が真である条件下で特徴ベクトル$X$が真である確率
- $P(X)$: 特徴ベクトル$X$が真である事前確率

ナイーブベイズ分類器は、ベイズの定理を用いて、与えられた特徴ベクトルに対して最も確率の高いクラスを推定する。ベイズの定理は以下のように表される

$$P(C_k|X) = \frac{P(X|C_k)P(C_k)}{P(X)}$$

ナイーブベイズ分類器では、特徴ベクトルの各要素が互いに独立であるという仮定（ナイーブベイズの仮定）を置く。この仮定により、条件付き確率$P(X|C_k)$は各特徴の条件付き確率の積として表現することができる。

$$P(X|C_k) = P(X_1, X_2, ..., X_n|C_k) = P(X_1|C_k)P(X_2|C_k)...P(X_n|C_k)$$

この仮定に基づいて、ナイーブベイズ分類器のクラスの推定は以下のように行われる

1. 各クラス$C_k$に対して、事前確率$P(C_k)$を推定する。これは、トレーニングデータセットにおける各クラスの出現頻度に基づいて求めることができる。

2. 各特徴$X_i$に対して、各クラス$C_k$における条件付き確率$P(X_i|C_k)$を推定する。これは、トレーニングデータセットにおいて各特徴の値がクラス$C_k$に属する事例に現れる頻度や確率分布を元に推定される。一般的に、連続値の場合は確率分布（例：ガウス分布）を使用し、カテゴリカルな値の場合は頻度や相対頻度を使用して推定する。

3. 推定された事前確率$P(C_k)$と条件付き確率$P(X_i|C_k)$を用いて、ベイズの定理を適用して事後確率$P(C_k|X)$を計算する。これにより、特徴ベクトル$X$が与えられた条件下で各クラスが真である確率が求められる。

4. 事後確率$P(C_k|X)$が最大となるクラス$C_k$を予測クラスとして選択する。つまり、与えられた特徴ベクトル$X$に対して最も確率が高いクラスを割り当てる。

ただし、ナイーブベイズ分類器は上記の仮定に基づいており、実際のデータセットにおいては仮定が成り立たない場合がある。そのため、データの事前分析や前処理が重要であり、他のより複雑なモデルと比較してパフォーマンスが制限されることもある。

## 確率分布

`scikit-learn` のナイーブベイズ分類器 (sklearn.naive_bayes) では、以下の確率分布を利用することができる。

- ベルヌーイ分布 (BernoulliNB)
  - バイナリ特徴をモデル化するために適しています。
  - 各特徴の値が0または1であり、出現する確率を表すパラメータ `alpha` を指定する
- 多項分布 (MultinomialNB)
  - カテゴリカル特徴をモデル化するために適しています。
  - 特徴の値が非負の整数であり、カテゴリの出現回数を表現します。
  - 特徴ベクトルに多項分布を仮定
  - `alpha` パラメータを指定してスムージングを制御します。
- ガウス分布 (GaussianNB)
  - 連続値の特徴をモデル化するために適する。
  - 特徴ベクトルに正規分布を仮定
  - 各クラスごとに特徴の平均と分散を推定し、尤度を計算する。

これらのクラスは `scikit-learn` の統一されたインターフェースに従っており、fitメソッドでモデルの学習、`predict` メソッドで予測を行うことができる。モデルのハイパーパラメータとしては、スムージングの強さや事前確率の設定などがある。

それぞれの確率分布は、データの特性と問題の要件に基づいて選択する必要がある。バイナリ特徴やカウントデータの場合はベルヌーイ分布や多項分布が適しており、連続値データの場合はガウス分布が一般的に使用される。データの特性やモデルの性能を考慮しながら、最適な確率分布を選択することが重要。

## 参考

- [ナイーブベイズ分類器を頑張って丁寧に解説してみる](https://qiita.com/aflc/items/13fe52243c35d3b678b0)
- [機械学習 〜 ナイーブベイズ分類器 〜](https://qiita.com/fujin/items/bd58fc7a93dc6e001045)
- [wikipedia: 単純ベイズ分類器](https://ja.wikipedia.org/wiki/%E5%8D%98%E7%B4%94%E3%83%99%E3%82%A4%E3%82%BA%E5%88%86%E9%A1%9E%E5%99%A8)
- [【機械学習×Python】ナイーブベイズ分類モデルの構築｜scikit-learnによる統計解析モデル開発実践](https://di-acc2.com/programming/python/8709/)